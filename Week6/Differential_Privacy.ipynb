{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Differential Privacy",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naraquev/Private-AI/blob/master/Week6/Differential_Privacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fett7JsXHNzC",
        "colab_type": "text"
      },
      "source": [
        "# **Differential Privacy Notebook Example.**\n",
        "\n",
        "This notebook implements the first Project (Week 6) of the Private AI Facebook Scholarship run by Udacity. \n",
        "\n",
        "The idea is to demonstrate the PATE analysis that comes from this paper: https://arxiv.org/pdf/1610.05755.pdf\n",
        "\n",
        "What are we doing? Suppose you are an organization with a DataSet of unlabeled data. You want to train an AI supervised model over this data, but the lack of labels is a problem. Some number X of organizations (Teachers) has data that could help you label your DataSet, but this data is private. \n",
        "\n",
        "We can train a model inside every organization, so we end up with X models that can then predict the data that our organization holds and give us the labels for our model. Even if the model is trained inside every organization, the definition of Differential Privacy states that it could be data leakage. So we perform a noisy mechanism over the X predictions to protect with  (ǫ, δ)-differential privacy the results. \n",
        "\n",
        "In the end, we compare the data independent and dependent Epsilon spent in the analysis and how different Hyperparameters can increase or reduce this difference.\n",
        "\n",
        "Greatly inspire by: https://github.com/dimun/pate_torch/blob/master/PATE.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCeaeD7aN1Ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPHYiKynKzU2",
        "colab_type": "text"
      },
      "source": [
        "# Here we define two classes:\n",
        "\n",
        "**The teacher** is going to be one organization with their data and their model. In theory, we could use different models in every organization. \n",
        "\n",
        "**The model** is the class that represents the models that we're performing. It's built in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSQutMrUN-nd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class teacher():\n",
        "  def __init__(self, dataL, device, batch_size = 32, epochs=5, print_every=120):\n",
        "    self.dataLoader = dataL\n",
        "    self.model = t_model()\n",
        "    self.criterion = nn.NLLLoss()\n",
        "    self.optimizer = optim.Adam(self.model.parameters(), lr=0.003)\n",
        "    self.epochs = epochs\n",
        "    self.dbug = print_every\n",
        "    self.device =  device\n",
        "                                           \n",
        "                                           \n",
        "                                           \n",
        "  def train(self):\n",
        "    self.model.to(self.device)\n",
        "    #print('Aqui voy!')\n",
        "    steps = 0\n",
        "    running_loss = 0\n",
        "    accuracy = 0\n",
        "    for e in range(self.epochs):\n",
        "        # Model in training mode, dropout is on\n",
        "        self.model.train()\n",
        "        accuracy=0\n",
        "        running_loss = 0\n",
        "        for images, labels in self.dataLoader:\n",
        "            \n",
        "            images, labels = images.to(self.device), labels.to(self.device)\n",
        "            steps += 1\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            output = self.model.forward(images)\n",
        "            loss = self.criterion(output, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()            \n",
        "            running_loss += loss.item()          \n",
        "            ps = torch.exp(output)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        #if(e == self.epochs -1 or e==0):\n",
        "         #   print(\"Epoch: {}/{}.. \".format(e+1, self.epochs),\n",
        "          #    \"Training Loss: {:.3f}.. \".format(running_loss/len(self.dataLoader)),              \n",
        "          #    \"Train Accuracy: {:.3f}\".format(accuracy/len(self.dataLoader)))\n",
        "    return accuracy/len(self.dataLoader)\n",
        "    \n",
        "  def eval(self, dataLoader):  \n",
        "    outputs = torch.zeros(0, dtype=torch.long).to(self.device)\n",
        "    self.model.to(self.device)\n",
        "    # Model in test mode, dropout is off\n",
        "    self.model.eval()\n",
        "    result=[]\n",
        "    for images, labels in dataLoader:\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        output = self.model.forward(images)\n",
        "        ps = torch.argmax(torch.exp(output), dim=1)\n",
        "        outputs = torch.cat((outputs, ps))\n",
        "    return outputs\n",
        "  \n",
        "  \n",
        "  \n",
        "class t_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(t_model, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AghY1hOFLY2M",
        "colab_type": "text"
      },
      "source": [
        "Then we import the MNIST dataset and define the number of teachers that we want to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHYrDQEjOH8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, ), (0.5,))])\n",
        "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "device =  torch.device(\"cuda:0\"\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_teachers = 100\n",
        "train_len = len(mnist_trainset)\n",
        "test_len = len(mnist_testset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SxjXVzfLxaO",
        "colab_type": "text"
      },
      "source": [
        "The function get_samples() subsets the data so every teacher has a unique and separate DataSet.\n",
        "\n",
        "The funciont create_teachers() creates the teachers with their own data and model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "470-uNzlOL2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_samples(num_teachers):\n",
        "  tam = len(mnist_trainset)\n",
        "  split= int(tam/num_teachers)\n",
        "  split_ini = split\n",
        "  indices = list(range(tam))\n",
        "  init=0\n",
        "  samples = []\n",
        "  for i in range(num_teachers):     \n",
        "    t_idx = indices[init:split]\n",
        "    t_sampler = SubsetRandomSampler(t_idx)\n",
        "    samples.append(t_sampler)\n",
        "    init = split\n",
        "    split = split+split_ini\n",
        "  return samples\n",
        "def create_teachers(samples):\n",
        "  teachers = []\n",
        "  for sample in samples:\n",
        "    loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=32, sampler=sample)\n",
        "    t = teacher(loader, device)\n",
        "    teachers.append(t)    \n",
        "  return teachers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaS6JikgOaJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = get_samples(num_teachers)\n",
        "teachers = create_teachers(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTZILVeaOtgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_teachers(teachers):\n",
        "  accuracy = []\n",
        "  for key, teacher in enumerate(teachers):\n",
        "    accuracy.append(teacher.train())\n",
        "    clear_output()\n",
        "    print('Teacher ', key)\n",
        "  return accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fz3iofHTMD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ac = train_teachers(teachers)\n",
        "print('The accuracy mean of all teachers is: ', np.mean(ac))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-TCXLqDL_Me",
        "colab_type": "text"
      },
      "source": [
        "eval_data() takes all the teachers and applies the trained models to our public data in order to obtein the labels.\n",
        "\n",
        "Function mechanism() applies Laplace mechanism to the predicted data that comes from the teachers in order to ensure (ǫ, δ)-differential privacy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX8iiVO-TKGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epsilon = 0.2\n",
        "def eval_data(teachers, mnist_testset):\n",
        "    preds = torch.torch.zeros((num_teachers, test_len), dtype=torch.long)\n",
        "    loader = torch.utils.data.DataLoader(mnist_testset, batch_size=32)\n",
        "    for key, teacher in enumerate(teachers):\n",
        "      clear_output()\n",
        "      print('Teacher:', key)      \n",
        "      result = teacher.eval(loader)\n",
        "      preds[key] = result\n",
        "    return preds.numpy()\n",
        "def mechanism(preds, epsilon=0.2):  \n",
        "  beta = 1 / epsilon\n",
        "  labels = np.array([]).astype(int)\n",
        "  for image_preds in np.transpose(preds):\n",
        "    label_counts = np.bincount(image_preds, minlength=10)\n",
        "    ori_label_counts = np.bincount(image_preds, minlength=10)\n",
        "    for i in range(10):\n",
        "      label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "    new_label = np.argmax(label_counts)\n",
        "    labels = np.append(labels, new_label)\n",
        "  return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-ifS_QuQ-N2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = eval_data(teachers, mnist_testset)\n",
        "labels = mechanism(preds, epsilon)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrjWBuKCO0lg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install syft\n",
        "from syft.frameworks.torch.differential_privacy import pate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLpRMC_xMg84",
        "colab_type": "text"
      },
      "source": [
        "Then we perform the PATE analysis. This analysis shows us that when for one observation the models have come to an agreement, is less the amount of data that could be leaked about the original observation that if the models are less in agreement.\n",
        "\n",
        "This means that for more teachers, better model architectures, more epochs, more data (In general, anything that could improve the accuracy of the individual models so they can have a better agreement) the data dependent Epsilon is going to be much less than the data independent one. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7-Pgj_vntqk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f6d17a97-812e-4567-d47d-ef9901207f3d"
      },
      "source": [
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=preds, indices=labels, noise_eps=epsilon, delta=1e-5)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 1611.5129254649705\n",
            "Data Dependent Epsilon: 34.607559293061705\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
